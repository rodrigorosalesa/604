---
title: "Final Project: Public Safety Measures and Public Health for Covid-19"
author: "Paul Croome, Rodrigo Rosales Alvarez, Ann Siddiqui and Kane Smith"
date: "2022-12-14"
output: 
  pdf_document: 
    toc_depth: 4
subtitle: "Data 603 - Statistical Modelling with Data"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

\begin{center}\textbf{} 
\\Professor: Dr. Paul Galpern
\\University of Calgary
\\Calgary, Alberta
\end{center}



\newpage
\tableofcontents



\newpage
# Introduction
The domain of our project covers healthcare-related indicators of the wellbeing of countries during the coronavirus disease 2019 (COVID-19) pandemic. In particular, we will be examining data related to the prevalence and severity of the COVID-19 pandemic and the governmental and societal measures taken to reduce the spread of the disease. These data were all daily reported between January 2020 and October 2022.

This is an interesting and important topic of study because, in our increasingly interconnected world, contagious diseases can be transmitted over vast distances remarkably easily. Even small, remote outbreaks of diseases anywhere in the world can swiftly turn into a global pandemic, which can then cause devastation on personal, societal, and worldwide scales. 


## Research Questions
1) What population-related metrics of countries around the world are most strongly related to the prevalence and severity of COVID-19 experienced in a country between February 2020 and October 2022 (as measured by average daily COVID-19 cases and deaths)? 

    a) What is the best model that can be built from these data for predicting the average daily new COVID-19 cases experienced in a country? 
    
    b) What is the best model that can be built from these data for predicting the average daily new COVID-19 deaths experienced in a country?

2) Among countries with reliably reported data relating to cases, positive test rates, vaccinations, and boosters, what societal and governmental responses to the COVID-19 pandemic are most strongly related to the prevalence and severity of COVID-19 experienced in a country between February 2020 and October 2022 (as measured by average daily COVID-19 cases and deaths)?

    a) What is the best model that can be built from these daily-reported data for predicting the daily new COVID-19 cases in a country?
    
    b) What is the best model that can be built from these daily-reported data for predicting the daily new COVID-19 deaths in a country?



\newpage
# Data Set Definition
The dataset we will use consists of diverse information related to the COVID-19 pandemic, including a country’s daily rates of COVID-19 diagnoses, hospitalizations, deaths, vaccinations, and booster shots. We will use features of these data to determine the prevalence and severity of the COVID-19 pandemic for each country . The dataset consists of daily information from January 1, 2020 to October 26, 2022 for more than 220 countries; each row corresponds to the Covid-19 information reported by an specific country in a certain date.

This dataset is in tabular form contained in a CSV file and is licensed for open access under the Creative Commons BY license. The dataset was put together by [Our World in Data](https://ourworldindata.org/coronavirus); more importantly, the data set is being updated daily by the same organization, for more information about the data pipeline and how the data set is being maintained click [here](https://docs.owid.io/projects/covid/en/latest/data-pipeline.html#overview).  



\newpage
# Methodology

To answer the questions outlined in our introduction, we will be creating various multiple linear regression models. Our analysis will follow the below structure:

**Preparation**
-Libraries Import
-Data Import
-Data Cleaning 
-Variable Definition 
-Data Preparation 

**Analysis**
- Question 1 Multiple Linear Regression Models
  - New Cases Model
    - New Cases Assumptions 
    - New Cases Box-Cox Transformation
    - Box-Cox Transformed Model Assumptions
  - New Deaths Model
    - New Deaths Assumptions
    - New Deaths Box-Cox Transformation 
- Question 2 Multiple Linear Regression Models
  - Model 1
    - Model 1 Assumptions 
    - Model 1 Box-Cox Transformation
    - Box-Cox Transformed Model Assumptions 
  - Model 2
    - Model 2 Assumptions
    - Model 2 Box-Cox Transformation
    - Box-Cox Transformed Model Assumptions 

**Results** 
-Question 1
  - New Cases Model
  - New Deaths Model
-Question 2
  - Model 1
  - Model 2

**Discussion** 
-Question 1
-Question 2

The above tasks were divided among the team in the following way: The preparation, including importing libraries, importing the data, cleaning the data, defining variables to be used in analysis, and wrangling the data, was all distributed evenly among the four team members. Analysis of Question 1 was divided between Rodrigo and Kane; this includes building the MLR models, checking the assumptions, doing any transformations and interpreting the results. Analysis of Question 2 was divided between Paul and Ann which also included building the MLR models, checking the assumptions, doing any transformations and interpreting the results.

# Preparation 
## Libraries 
```{r, results='hide', echo=FALSE, include=FALSE}
library(dplyr) # data manipulation
library(ggplot2) # graphs
library(GGally) # extension to ggplot2
library(mosaic) # statistics and calculus functions
library(tidyverse) # data science 
library(tidyr) # tidy data
library(olsrr) # teaching and learning OLS regression
library(leaps) # best subset of variables for a model
library(mctest) # VIF
library(car) # VIF
library(lmtest) # Bracuh-Pagan
library(agricolae) # Newman-Keuls
library(MASS) # box-cox
options(scipen=999) # show results without scientific notation
```


## Data Import
As our data is in CSV format, we simple use the function **read_csv()** to import our file into a data frame.
```{r}
covid_raw <- read_csv('data.csv', show_col_types = FALSE)
```


## Data Cleaning 
Many countries and facilities are under reporting Covid-19 statistics like cases and deaths, according to Claire Klobucista from Council of Foreign Relations. This is could be catastrophically as government could not respond accordingly to the real situation. For our research this is very important as well, if we put bad data into our model, we would create a bad model. To solve this problem we will remove from our dataset the countries that are present in the bottom 5% of number of new cases smoothed per million and number of new deaths smoothed per million. 
```{r}
# Getting countries with bottom 5% of new_cases_smoothed_per_million
tenth_percentile_cases <- quantile(covid_raw$new_cases_smoothed_per_million, probs = 0.05, na.rm = TRUE)
bad_country_cases <- covid_raw[covid_raw$new_cases_smoothed_per_million < tenth_percentile_cases,]
bad_country_list_cases <- unique(bad_country_cases$location)

# Getting countries with bottom 5% of new_deaths_smoothed_per_million
tenth_percentile_deaths <- quantile(covid_raw$new_deaths_smoothed_per_million, probs = 0.05, na.rm = TRUE)
bad_country_deaths <- covid_raw[covid_raw$new_deaths_smoothed_per_million < tenth_percentile_deaths,]
bad_country_list_deaths <- unique(bad_country_deaths$location)

# Remove countries that appear in either above lists
bad_country_list <- append(bad_country_list_deaths, bad_country_list_cases)
good_countries <-   covid_raw[!covid_raw$location %in% bad_country_list, ]

covid_data <- good_countries
```

More cleaning tasks were missing, for starters we generated a new column called *smokers* that was the average of *female_smokers* and *male_smokers*, after that we replaced all the null values for 0s, as we can´t assign a number to a factor, we decided to drop continent and iso_code, columns, test_unitsand date as are not important for our analysis.
```{r}
# Creating the column "smokers"
covid_data$smokers <- (covid_data[['male_smokers']] + covid_data[['female_smokers']]) / 2

# Drop column continent
covid = subset(covid_data, select = -c(iso_code, continent, tests_units, date) )

# Changing Null Values to 0s
covid[is.na(covid)] = 0
```


## Variable Definition.

**Independent Variable** 

- _new_cases_: new confirmed cases of COVID-19. Continuous Variable.
- _new_deaths_: new deaths attributed to COVID-19. Continuous Variable.


**Dependent Variables**

1) Population metrics

    - _extreme_poverty_: The number of the population per million that is considered to be in extreme poverty.
    - _gdp_per_capita_: GDP per capita of the country.
    - _median_age_: Median age of the population.
    - _population_: Number of people in the country.
    - _human_development_index_: Human development index as calculated by [Human Development Reports](https://hdr.undp.org/data-center/human-development-index#/indicies/HDI).
    - _population_density_: Population density of the country.
    - _aged_65_older_: The number of the population per million that is aged over 65 years old.
    

2) Health metrics

    - _cardiovasc_death_rate_: The death rate caused by cardiovascular disease.
    - _diabetes_prevalence_: The number of the population per million that is diagnosed with diabetes.
    - _life_expectancy_: The life expectancy of the population of a country.
    - _reproduction_rate_: The rate of reproduction of the population of a country.
    - _smokers_: The number of the population per million that smokes cigarettes.
    

3) COVID metrics

    - _stringency_index_: A measure of how stringent the policies related to controlling the spread of COVID is.
    - _hosp_patients_: The number of people hospitalized due to COVID.
    - _new_tests_: The number of new COVID tests conducted in a day.


## Data Preparation 
To create the finala dataset that will be used to create the *New Cases Model* and *New Deaths Model* we performed an aggregation function (mean) across the variable country as we are only interested in having one data point per country. The new data point will be the mean of every other variable present in the table; we decided to use the mean as that is the best way to aggregate the variables that we are interested on using like new cases, new deaths, population, stringency index and more.
```{r}
covid_agg <- covid %>% group_by(location) %>% summarise(across(everything(), mean), .groups = 'drop') %>% as.data.frame()
```

#Analysis

## Multiple Linear Regression Models for Research Question 1
Our research question number 1 is:

1) What population-related metrics of countries around the world are most strongly related to the prevalence and severity of COVID-19 experienced in a country between February 2020 and October 2022 (as measured by average daily COVID-19 cases and deaths)? 

    a) What is the best model that can be built from these data for predicting the average daily new COVID-19 cases experienced in a country? 
    
    b) What is the best model that can be built from these data for predicting the average daily new COVID-19 deaths experienced in a country?
    
For this reason we are going to build two Multiple Linear Regressions Models, one for New Cases and one for New Deaths, using the tools we learnt during class.

It is important to state that for all the statistical tests that we will perform trhoughout this project we will use a value of $\alpha = 0.05$.


### New Cases Model

We started by defining our full model, including all the variables that make senses to predict New Covid-19 Cases for a specific country. Using the summary() function we are able to see the most important information about our model. 

```{r}
model_cases_full = lm(new_cases ~ aged_65_older + smokers + cardiovasc_death_rate + diabetes_prevalence + extreme_poverty + gdp_per_capita + median_age + life_expectancy + population + stringency_index + human_development_index + reproduction_rate + hosp_patients + new_tests + population_density, data=covid_agg)

summary(model_cases_full)
```

Using the *Step Wise Regression Procedure* to create the best fit additive model we found out that only 3 variables were significant; for these 3 terms we can reject reject the Null Hypothesis for the individual coefficient t test, meaning that the terms are significant and the coefficients should be different than 0. The 3 terms that passed the mentioned test are: **population**, **hosp_patients**, **life expectancy**.

Hypothesis Testing for Individual Coefficient t tests.\
$H_0: \beta_i = 0$\
$H_a: \beta_i \neq 0$\
$(i = \text{population, hosp_patients, life_expectancy})$

```{r}
model_cases_stepwise = ols_step_both_p(model_cases_full, pent=0.1, prem=0.3, progress=TRUE)
```

The model obtained after performing the *Step Wise Regression Procedure* is:\
$\widehat{y}_{new.cases} = 14508.9677 + 0.0001 x_{population} + 1.7133 x_{hosp.patients} - 192.6691 x_{life.expectancy}$

Adjusted R Squared of our model is: 0.6839, meaning that the proportion of the total variation that is explained by the model is 68.39%.

```{r}
model_cases_revised = lm(new_cases ~ population + hosp_patients + life_expectancy, data=covid_agg)
summary(model_cases_revised)
```

Using interactions terms can be a powerful way to increase the efficiency of our model as sometimes the effect of an independent variable on a dependent variable, in our case new Covid-19 cases, is not constant over all of the values of the other independent variables. As population is on our terms we decided to include interactions terms as depending on the total population of a country all the other variables could change.

Hypothesis Testing for Individual Coefficient t tests.\
$H_0: \beta_i = 0$\
$H_a: \beta_i \neq 0$\
$(i = \text{(population, hosp_patients, life_expectancy)^2})$

We failed to reject our Null Hypothesis for the interaction term *population:hosp_patients* suggesting it should not be including in our model, for all the other interaction terms we rejected the Null Hypothesis and we will conserve those terms in the model. 

```{r}
model_cases_interactions = lm(new_cases ~ (population + hosp_patients + life_expectancy)^2, data=covid_agg)
summary(model_cases_interactions)
```

Here below, the interaction model.\
$\widehat{y}_{new.cases} = 33563.8914 +0.00004 x_{population} - 5.3334 x_{hosp.patients} - 460.8215 x_{life.expectancy} + 0.0000004 x_{population} x_{life.expectancy} + 0.0905 x_{hosp.patients} x_{life.expectancy}$

With the addition of the interactions terms the Adjusted R Squared of our model is now 0.7202, meaning that the proportion of the total variation that is explained by the model is 72.02%.

```{r}
model_cases_interactions_2 = lm(new_cases ~ population + hosp_patients + life_expectancy + population:life_expectancy + hosp_patients:life_expectancy, data=covid_agg)

summary(model_cases_interactions_2)
```

We would like to try a higher order term to see if we can improve our Adjusted R Squared, as some of the relations with the dependent variable could be curvature instead of linear. To check this relations we will use the ggpairs to plot the correlation between the independent variables and the dependent variables. 

```{r}
covid_higher_order <-data.frame(covid_agg$new_cases, covid_agg$population, covid_agg$hosp_patients, covid_agg$life_expectancy)

ggpairs(covid_higher_order, lower = list(continuous="smooth_loess", combo="facethist", discrete="facetbar", na="na"))
```

Right away we can observe that there is a curvature relationship between life_expentancy and new_cases, for that reason we decided to try a square term to see if the model behaves better. 

Hypothesis Testing for Individual Coefficient t tests.\
$H_0: \beta_i = 0$\
$H_a: \beta_i \neq 0$\
$(i = \text{(population, hosp_patients, life_expectancy)^2})$

We rejected the Null Hypothesis for the squared term, life_expentancy, so we can include it in the model. There was an increase of 0.0068 in our Adjusted R Squared, the new value is 0.727, meaning that we can now explain more of the variation of the dependent variable.

```{r}
model_cases_squared = lm(new_cases ~ population + hosp_patients + life_expectancy + I(life_expectancy^2) + population:life_expectancy + hosp_patients:life_expectancy, data=covid_agg)

summary(model_cases_squared)
```

As the increase in Adjusted R Squared was so little we decided to stop here and use this model as our Best Fit Model.\
$\widehat{y}_{new.cases} = 40658.4854 + 0.00004 x_{population} - 6.0590 x_{hosp.patients} - 1338.9337 x_{life.expectancy} + 10.4609 x^2_{life.expectancy} + 0.0000005 x_{population} x_{life.expectancy} + 0.0969 x_{hosp.patients} x_{life.expectancy}$


### New Cases Assumptions 

#### Normality Assumption 

Testing for normality using the Shapiro-Wilk test:

**Null hypothesis:** the sample data are significantly normally distributed

**Alternative hypothesis:** the sample data are not significantly normally distributed

We will set the alpha value to 0.05.

```{r}
shapiro.test(residuals(model_cases_squared))
```


Plotting a Q-Q plot:

```{r}
ggplot(model_cases_squared, aes(sample=model_cases_squared$residuals)) +
stat_qq() +
stat_qq_line()
```

From the output of our test, we get a p-value of 0.00000000000000022 which is less than 0.05. This means we can reject our null hypothesis that our sample data is significantly normally distributed and conclude with a significance level of 0.05 that our sample data is not normally distributed. This means that there is a problem with the normality assumption.

#### Homoscedasticity Assumption

Testing for heteroscedasticity using the Breusch-Pagan test:

**Null hypothesis:** heteroscedasticity is not present ($H_0: \sigma^2_1 = \sigma^2_2 =... = \sigma^2_n$)

**Alternative hypothesis:** heteroscedasticity is present ($H_a:$ at least one $\sigma^2_i$ is different from the others)

We will set the alpha value to 0.05.

```{r}
bptest(model_cases_squared)
```

From the output of our test, we get a p-value of 0.0000001319 which is less than 0.05. This means we can reject the null hypothesis that there is homoscedasticity and conclude with a significance level of 0.05 that our model is not homoscedastic. This means that there is a problem with the homoscedasticity assumption. 

#### Linearity Assumption 

Testing for linearity using a residuals vs predicted $\hat{Y}$ plot:
```{r}
ggplot(model_cases_squared, aes(x=.fitted, y=.resid)) +
geom_point() + geom_smooth()+
geom_hline(yintercept = 0)
```

Looking at the plot above, our model does seem to be linear.

#### Outliers Assumption

Testing for outliers using leverage:
```{r}
lev=hatvalues(model_cases_squared)
p = length(coef(model_cases_squared))
n = nrow(covid_agg)
outlier3p = lev[lev>(3*p/n)]
outlier3p
```

```{r}
plot(model_cases_squared,which=5)
```

From our output, there seems to be two data points, row 12, 44, 64, 76, 89, 99, 101, 107, and 110 with a leverage greater than $3p/n$. 

Removing the rows that are influential outliers:
```{r}
covid_agg2 <- covid_agg[-c(12, 44, 64, 73, 76, 87, 89, 99, 101, 107, 110), ]
```


#### Independant Errors Assumption

Plotting residuals vs. fitted values to see if the error terms look independant from each other:
```{r}
ggplot(model_cases_squared, aes(x=.fitted, y=.resid)) +
geom_point() + geom_smooth()+
geom_hline(yintercept = 0)
```

Looking at the plot above, it seems that the residuals are evenly distributed, meaning that the residuals are independent from each other.

Since our model failed majority of the assumptions, we will try a Box-Cox transformation to see if it improves at all.

First, we should confirm that our response variable is always positive.

```{r}
covid_agg2[covid_agg2["new_cases"]<=0,]

# New model without outliers 
model_cases_squared2 = lm(new_cases ~ population + hosp_patients + life_expectancy + I(life_expectancy^2) + population:life_expectancy + hosp_patients:life_expectancy, data=covid_agg2)
```

We get no rows where new cases is less than 0, so we can continue with the Box-Cox transformation.

```{r}
bc=boxcox(model_cases_squared2,lambda=seq(-10,10))
bestlambda=bc$x[which(bc$y==max(bc$y))]
bestlambda
```

We get a lambda of -0.1010101. Conducting the Box-Cox transformation:

```{r}
bc_model_cases_squared2 = lm((((new_cases^(-0.1010101))-1)/-0.1010101)~ population + hosp_patients + life_expectancy + I(life_expectancy^2) + population:life_expectancy + hosp_patients:life_expectancy,data=covid_agg2)
```

After conducting a Box-Cox transformation, our model has an adjusted R-squared value of 48.31% and an RMSE of 1.019. This is significantly worse performance when compared to our final model before doing a Box-Cox transformation, however, we will check if the assumptions are met with our new model.

### Assumptions for Box-Cox Transoformed New Cases Model

#### Normailty Assumption 

Testing for normality using the Shapiro-Wilk test:

**Null hypothesis:** the sample data are significantly normally distributed

**Alternative hypothesis:** the sample data are not significantly normally distributed

We will set the alpha value to 0.05.

```{r}
shapiro.test(residuals(bc_model_cases_squared2))
```

Plotting a Q-Q plot:

```{r}
ggplot(bc_model_cases_squared2, aes(sample=bc_model_cases_squared2$residuals)) +
stat_qq() +
stat_qq_line()
```

From the output of our test, we get a p-value of 0.005933 which is less than 0.05. This means we can reject our null hypothesis that our sample data is significantly normally distributed and conclude with a significance level of 0.05 that our sample data is not normally distributed. This means that there is a problem with the normality assumption.

#### Homoscedasticity Assumption

Testing for heteroscedasticity using the Breusch-Pagan test:

**Null hypothesis:** heteroscedasticity is not present ($H_0: \sigma^2_1 = \sigma^2_2 =... = \sigma^2_n$)

**Alternative hypothesis:** heteroscedasticity is present ($H_a:$ at least one $\sigma^2_i$ is different from the others)

We will set the alpha value to 0.05.

```{r}
bptest(bc_model_cases_squared2)
```

From the output of our test, we get a p-value of 0.01903 which is less than 0.05. This means we can reject the null hypothesis that there is homoscedasticity and conclude with a significance level of 0.05 that our model is not homoscedastic. This means that there is a problem with the homoscedasticity assumption. 

#### Linearity Assumption 

Testing for linearity using a residuals vs predicted $\hat{Y}$ plot:
```{r}
ggplot(bc_model_cases_squared2, aes(x=.fitted, y=.resid)) +
geom_point() + geom_smooth()+
geom_hline(yintercept = 0)
```

Looking at the plot above, our model does seem to be linear.

#### Independant Errors Assumption

Plotting residuals vs. fitted values to see if the error terms look independent from each other:
```{r}
ggplot(bc_model_cases_squared2, aes(x=.fitted, y=.resid)) +
geom_point() + geom_smooth()+
geom_hline(yintercept = 0)
```

Our Box-Cox transformed model still fails the same assumptions that our original final model did. However, it performs better in the tests, meaning that the Box-Cox transformation did help. 


### New Deaths Model

For the New Deaths model we followed the same procedure. We started by defining our full model, including all the variables that make senses to predict New Covid-19 Deaths for a specific country. Using the summary() function we are able to see the most important information about our model.

```{r}
model_deaths_full = lm(new_deaths ~ aged_65_older + smokers + cardiovasc_death_rate + diabetes_prevalence + extreme_poverty + gdp_per_capita + median_age + life_expectancy + population + stringency_index + human_development_index + reproduction_rate + hosp_patients + new_tests + population_density, data=covid_agg)

summary(model_deaths_full)
```

Using the *Step Wise Regression Procedure* to create the best fit additive model we found out that only 3 variables were significant; for these 3 terms we can reject reject the Null Hypothesis for the individual coefficient t test, meaning that the terms are significant and the coefficients should be different than 0. The 3 terms that passed the mentioned test are: **population**, **hosp_patients**, **life expectancy**.

Hypothesis Testing for Individual Coefficient t tests.\
$H_0: \beta_i = 0$\
$H_a: \beta_i \neq 0$\
$(i = \text{population, hosp_patients, life_expectancy})$

```{r}
model_deaths_stepwise = ols_step_both_p(model_deaths_full, pent=0.1, prem=0.3, progress=TRUE)
```

The model obtained after performing the *Step Wise Regression Procedure* is:\
$\widehat{y}_{new.deaths} = 130.0567 + 0.0000007 x_{population} + 0.0168 x_{hosp.patients} - 1.7261 x_{life.expectancy}$

Adjusted R Squared of our model is: 0.7582, meaning that the proportion of the total variation that is explained by the model is 75.82%.

```{r}
model_deaths_revised = lm(new_deaths ~ population + hosp_patients + life_expectancy, data=covid_agg)
summary(model_deaths_revised)
```

Using interactions terms can be a powerful way to increase the efficiency of our model as sometimes the effect of an independent variable on a dependent variable, in our case new Covid-19 deaths, is not constant over all of the values of the other independent variables. As population is on our terms we decided to include interactions terms as depending on the total population of a country all the other variables could change.

Hypothesis Testing for Individual Coefficient t tests.\
$H_0: \beta_i = 0$\
$H_a: \beta_i \neq 0$\
$(i = \text{(population, hosp_patients, life_expectancy)^2})$

We failed to reject our Null Hypothesis for the interaction term *population:hosp_patients* suggesting it should not be including in our model, for all the other interaction terms we rejected the Null Hypothesis and we will conserve those terms in the model. 

```{r}
model_deaths_interactions = lm(new_deaths ~ (population + hosp_patients + life_expectancy)^2, data=covid_agg)
summary(model_deaths_interactions)
```

Here below, the interaction model.\
$\widehat{y}_{new.deaths} = 323.2947 + 0.0000004 x_{population} - 0.0472 x_{hosp.patients} - 4.4463 x_{life.expectancy} + 0.000000005 x_{population} x_{life_expectancy} + 0.0008 x_{hosp.patients} x_{life.expectancy}$

With the addition of the interactions terms the Adjusted R Squared of our model is now 0.7982, meaning that the proportion of the total variation that is explained by the model is 79.82%.

```{r}
model_deaths_interactions_2 = lm(new_deaths ~ population + hosp_patients + life_expectancy + population:life_expectancy + hosp_patients:life_expectancy, data=covid_agg)

summary(model_deaths_interactions_2)
```

We would like to try a higher order term to see if we can improve our Adjusted R Squared, as some of the relations with the dependent variable could be curvature instead of linear. To check this relations we will use the ggpairs to plot the correlation between the independent variables and the dependent variables. 

```{r}
covid_higher_order <-data.frame(covid_agg$new_deaths, covid_agg$population, covid_agg$hosp_patients, covid_agg$life_expectancy)

ggpairs(covid_higher_order, lower = list(continuous="smooth_loess", combo="facethist", discrete="facetbar", na="na"))
```

Right away we can observe that there is a curvature relationship between life_expentancy and new_deaths, for that reason we decided to try a square term to see if the model behaves better. 

Hypothesis Testing for Individual Coefficient t tests.\
$H_0: \beta_i = 0$\
$H_a: \beta_i \neq 0$\
$(i = \text{(population, hosp_patients, life_expectancy)^2})$

We rejected the Null Hypothesis for the squared term, life_expentancy, so we can include it in the model. There was an increase of 0.0043 in our Adjusted R Squared, the new value is 0.8025, meaning that we can now explain more of the variation of the dependent variable.

```{r}
model_deaths_squared = lm(new_deaths ~ population + hosp_patients + life_expectancy + I(life_expectancy^2) + population:life_expectancy + hosp_patients:life_expectancy, data=covid_agg)

summary(model_deaths_squared)
```

As the increase in Adjusted R Squared was so little we decided to stop here and use this model as our Best Fit Model.\
$\widehat{y}_{new.deaths} = 379.8272 + 0.0000004 x_{population} - 0.0530 x_{hosp.patients} - 11.4435 x_{life.expectancy} + 0.0834 x^2_{life.expectancy} + 0.000000005 x_{population} x_{life.expectancy} + 0.00087 x_{hosp.patients} x_{life.expectancy}$

### Assumptions for New Deaths Model

#### Normality Assumption 

Testing for normality using the Shapiro-Wilk test:

**Null hypothesis:** the sample data are significantly normally distributed

**Alternative hypothesis:** the sample data are not significantly normally distributed

We will set the alpha value to 0.05.

```{r}
shapiro.test(residuals(model_deaths_squared))
```

Plotting a Q-Q plot:

```{r}
ggplot(model_deaths_squared, aes(sample=model_deaths_squared$residuals)) +
stat_qq() +
stat_qq_line()
```

From the output of our test, we get a p-value of 0.00000000000000022 which is less than 0.05. This means we can reject our null hypothesis that our sample data is significantly normally distributed and conclude with a significance level of 0.05 that our sample data is not normally distributed. This means that there is a problem with the normality assumption.

#### Homoscedasticity Assumption 

Testing for heteroscedasticity using the Breusch-Pagan test:

**Null hypothesis:** heteroscedasticity is not present ($H_0: \sigma^2_1 = \sigma^2_2 =... = \sigma^2_n$)

**Alternative hypothesis:** heteroscedasticity is present ($H_a:$ at least one $\sigma^2_i$ is different from the others)

We will set the alpha value to 0.05.

```{r}
bptest(model_deaths_squared)
```

From the output of our test, we get a p-value of 0.00000000877 which is less than 0.05. This means we can reject the null hypothesis that there is homoscedasticity and conclude with a significance level of 0.05 that our model is homoscedastic. This means that there is a problem with the homoscedasticity assumption. 

#### Linearity Assumption 

Testing for linearity using a residuals vs predicted $\hat{Y}$ plot:
```{r}
ggplot(model_deaths_squared, aes(x=.fitted, y=.resid)) +
geom_point() + geom_smooth()+
geom_hline(yintercept = 0)
```

Looking at the plot above, our model does seem to be linear.

#### Outliers Assumption

Testing for outliers using leverage:
```{r}
lev=hatvalues(model_deaths_squared)
p = length(coef(model_deaths_squared))
n = nrow(covid_agg)
outlier3p = lev[lev>(3*p/n)]
outlier3p
```

```{r}
plot(model_deaths_squared,which=5)
```

From our output, there seems to be two data points, row 12, 44, 64, 76, 89, 99, 101, 107, and 110 with a leverage greater than $3p/n$. 

Since the rows that are influential outliers are the same for our cases model, we will use the same dataset with removed outliers (covid_agg2)

#### Independant Errors Assumption 

Plotting residuals vs. fitted values to see if the error terms look independent from each other:
```{r}
ggplot(model_deaths_squared, aes(x=.fitted, y=.resid)) +
geom_point() + geom_smooth()+
geom_hline(yintercept = 0)
```

Looking at the plot above, it seems that the residuals are evenly distributed, meaning that the residuals are independant from each other.

```{r}
covid_agg2[covid_agg2["new_deaths"]<=0,]
model_deaths_squared2 = lm(new_deaths ~ population + hosp_patients + life_expectancy + I(life_expectancy^2) + population:life_expectancy + hosp_patients:life_expectancy, data=covid_agg2)
```

We get no rows where new cases is less than 0, so we can continue with the Box-Cox transformation.

```{r}
bc=boxcox(model_deaths_squared2,lambda=seq(-10,10))
bestlambda=bc$x[which(bc$y==max(bc$y))]
bestlambda
```

We get a lambda of -0.1010101. Conducting the Box-Cox transformation:

```{r}
# Getting error when running lm about having Inf in the data frame. 
bc_model_deaths_final2 = lm((((new_deaths^(-0.1010101))-1)/-0.1010101)~ population + hosp_patients + life_expectancy + I(life_expectancy^2) + population:life_expectancy + hosp_patients:life_expectancy,data=covid_agg2)
```

We will now re-check the model assumption for our Box-Cox transformed model.

### Assumptions for Box-Cox Transoformed New Deaths Model

#### Normailty Assumption 

Testing for normality using the Shapiro-Wilk test:

**Null hypothesis:** the sample data are significantly normally distributed

**Alternative hypothesis:** the sample data are not significantly normally distributed

We will set the alpha value to 0.05.

```{r}
shapiro.test(residuals(bc_model_deaths_final2))
```

Plotting a Q-Q plot:

```{r}
ggplot(bc_model_deaths_final2, aes(sample=bc_model_deaths_final2$residuals)) +
stat_qq() +
stat_qq_line()
```

From the output of our test, we get a p-value of 0.0001203 which is less than 0.05. This means we can reject our null hypothesis that our sample data is significantly normally distributed and conclude with a significance level of 0.05 that our sample data is not normally distributed. This means that there is a problem with the normality assumption.

#### Homoscedasticity Assumption

Testing for heteroscedasticity using the Breusch-Pagan test:

**Null hypothesis:** heteroscedasticity is not present ($H_0: \sigma^2_1 = \sigma^2_2 =... = \sigma^2_n$)

**Alternative hypothesis:** heteroscedasticity is present ($H_a:$ at least one $\sigma^2_i$ is different from the others)

We will set the alpha value to 0.05.

```{r}
bptest(bc_model_deaths_final2)
```

From the output of our test, we get a p-value of 0.1908 which is greater than 0.05. This means we fail reject the null hypothesis that there is homoscedasticity and conclude with a significance level of 0.05 that our model is not homoscedastic. This means that there is no problem with the homoscedasticity assumption. 

#### Linearity Assumption 

Testing for linearity using a residuals vs predicted $\hat{Y}$ plot:
```{r}
ggplot(bc_model_deaths_final2, aes(x=.fitted, y=.resid)) +
geom_point() + geom_smooth()+
geom_hline(yintercept = 0)
```

Looking at the plot above, our model does seem to be some what linear.

#### Independant Errors Assumption

Plotting residuals vs. fitted values to see if the error terms look independent from each other:
```{r}
ggplot(bc_model_deaths_final2, aes(x=.fitted, y=.resid)) +
geom_point() + geom_smooth()+
geom_hline(yintercept = 0)
```

Our Box-Cox transformed model still fails the some of the assumptions that our original final model did. However, it ended up passing the Breusch-Pagan test for homoscedasticity. Since our Box-Cox model passes more assumptions that our original model, we will use it instead. That means our new model is:
```{r}
summary(bc_model_deaths_final2)
```

$\widehat{y}_{new.deaths} = 4.6588 + 0.0000000002 x_{population} + 0.007286 x_{hosp.patients} - 0.19199 x_{life.expectancy} + 0.001649 x^2_{life.expectancy} + 0.0000000004   x_{population} x_{life.expectancy} -0.00008 x_{hosp.patients} x_{life.expectancy}$

From this model we get an Adjusted R Squared of our model is now 0.3707, meaning that the proportion of the total variation that is explained by the model is 37.07%. This is significantly worse when compared to our final mode before Box-Cox transformation of 79.82% but since our Box-Cox model passes more assumptions, we can rely more on it's predictions.

\newpage
# Results



\newpage
# Discussion



\newpage
# References
Klobucista, Claire (2021, May 10). By How Much Are Countries Underreporting COVID-19 Cases and Deaths?. Council of Foreign Relations. https://www.cfr.org/in-brief/how-much-are-countries-underreporting-covid-19-cases-and-deaths


